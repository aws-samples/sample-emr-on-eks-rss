# NOTE: Make sure you replace <ENTER_S3_BUCKET> with your S3 Bucket before running this job.
# To execute run this command -> kubectl apply -f celeborn-benchmark.yml
---
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: celeborn-tpcds-benchmark
  namespace: emr-data-team-a
spec:
  type: Scala
  mode: cluster
  # EMR optimized runtime image with Celeborn support
  image: "public.ecr.aws/dong-registry/celeborn-emr-eks:emr-6.10.0"
  imagePullPolicy: Always
  mainClass: com.amazonaws.eks.tpcds.BenchmarkSQL
  mainApplicationFile: local:///usr/lib/spark/examples/jars/eks-spark-benchmark-assembly-1.0.jar
  arguments:
    # Security Fix: Added documentation for placeholder values
    # Replace <S3_BUCKET> with your S3 bucket name (e.g., my-company-data-bucket)
    - "s3://<S3_BUCKET>/TPCDS-TEST-3T"
    - "s3://<S3_BUCKET>/TPCDS-TEST-3T-RESULT"
    - "/opt/tpcds-kit/tools"
    - "parquet"
    - "3000"
    - "1"
    - "false"
    - "q24a-v2.4,q25-v2.4,q26-v2.4,q27-v2.4,q30-v2.4,q31-v2.4,q32-v2.4,q33-v2.4,q34-v2.4,q36-v2.4,q37-v2.4,q39a-v2.4,q39b-v2.4,q41-v2.4,q42-v2.4,q43-v2.4,q52-v2.4,q53-v2.4,q55-v2.4,q56-v2.4,q60-v2.4,q61-v2.4,q63-v2.4,q67-v2.4,q73-v2.4,q77-v2.4,q83-v2.4,q86-v2.4,q98-v2.4"
    - "true"
  hadoopConf:
    # EMRFS filesystem config
    fs.s3.customAWSCredentialsProvider: "com.amazonaws.auth.WebIdentityTokenCredentialsProvider"
    fs.s3.impl: "com.amazon.ws.emr.hadoop.fs.EmrFileSystem"
    fs.AbstractFileSystem.s3.impl: "org.apache.hadoop.fs.s3.EMRFSDelegate"
    fs.s3.buffer.dir: "/mnt/s3"
    fs.s3.getObject.initialSocketTimeoutMilliseconds: "2000"
    mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem: "2"
    mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem: "true"
  sparkConf:
    # -----------------------------------------------------
    # Kubernetes timeout configurations
    spark.kubernetes.local.dirs.tmpfs: "true"
    # Security Fix: Reduced from 16.7 hours to reasonable 5-minute timeouts
    spark.kubernetes.submission.connectionTimeout: "300000"
    spark.kubernetes.submission.requestTimeout: "300000"
    spark.kubernetes.driver.connectionTimeout: "300000"
    spark.kubernetes.driver.requestTimeout: "300000"
    spark.network.timeout: "2000s"
    spark.executor.heartbeatInterval: "300s"
    # -----------------------------------------------------

    # Event logging
    spark.eventLog.enabled: "true"
    # Security Fix: Parameterized bucket name to prevent bucket sniping vulnerability
    spark.eventLog.dir: "s3://<EVENT_LOG_BUCKET>/spark-events/"
    spark.kubernetes.driver.pod.name: "driver-celeborn-tpcds-benchmark"
    spark.kubernetes.executor.podNamePrefix: "clb-dra-track"

    # Required for EMR Runtime and Glue Catalogue
    spark.driver.extraClassPath: /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/home/hadoop/extrajars/*
    spark.driver.extraLibraryPath: /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native
    spark.executor.extraClassPath: /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/home/hadoop/extrajars/*
    spark.executor.extraLibraryPath: /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native
    # EMRFS committer
    spark.sql.parquet.output.committer.class: "com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter"
    spark.sql.parquet.fs.optimized.committer.optimization-enabled: "true"
    spark.sql.emr.internal.extensions: "com.amazonaws.emr.spark.EmrSparkSessionExtensions"

    # Memory and performance configurations
    # Security Fix: Increased driver memory overhead for better balance
    spark.driver.memoryOverhead: "2G"
    spark.executor.memoryOverhead: "2G"
    spark.executor.defaultJavaOptions: "-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:OnOutOfMemoryError='kill -9 %p'"
    spark.driver.defaultJavaOptions: "-XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70"

    # Serialization
    spark.serializer: "org.apache.spark.serializer.KryoSerializer"

    # Dynamic Resource Allocation (DRA) configurations
    spark.shuffle.service.enabled: "false"
    spark.dynamicAllocation.enabled: "true"
    spark.dynamicAllocation.shuffleTracking.enabled: "true"
    spark.dynamicAllocation.minExecutors: "1"
    spark.dynamicAllocation.maxExecutors: "50"
    # Security Fix: Increased from 5s to 30s for better stability and reduced executor churn
    spark.dynamicAllocation.executorIdleTimeout: "30s"
    spark.dynamicAllocation.shuffleTracking.timeout: "30s"
    spark.dynamicAllocation.executorAllocationRatio: "0.5"

    # Adaptive Query Execution (AQE) configurations
    spark.sql.adaptive.localShuffleReader.enabled: "false"

    # Celeborn Remote Shuffle Service (RSS) configurations
    spark.shuffle.manager: "org.apache.spark.shuffle.celeborn.SparkShuffleManager"
    spark.celeborn.master.endpoints: "celeborn-master-0.celeborn-master-svc.celeborn:9097,celeborn-master-1.celeborn-master-svc.celeborn:9097,celeborn-master-2.celeborn-master-svc.celeborn:9097"
    spark.shuffle.sort.io.plugin.class: "org.apache.spark.shuffle.sort.io.LocalDiskShuffleDataIO"
    spark.celeborn.client.spark.push.unsafeRow.fastWrite.enabled: "false"
    spark.celeborn.shuffle.chunk.size: "4m"
    spark.celeborn.client.push.maxReqsInFlight: "128"
    spark.celeborn.rpc.askTimeout: "240s"
    spark.celeborn.client.push.replicate.enabled: "false"
    spark.celeborn.client.push.excludeWorkerOnFailure.enabled: "true"
    spark.celeborn.client.fetch.excludeWorkerOnFailure.enabled: "true"
    spark.celeborn.client.commitFiles.ignoreExcludedWorker: "true"

    # Prometheus metrics configurations
    spark.metrics.appStatusSource.enabled: "true"
    spark.ui.prometheus.enabled: "true"
    spark.executor.processTreeMetrics.enabled: "true"
    spark.kubernetes.driver.annotation.prometheus.io/scrape: "true"
    spark.kubernetes.driver.annotation.prometheus.io/path: "/metrics/executors/prometheus/"
    spark.kubernetes.driver.annotation.prometheus.io/port: "4040"
    spark.kubernetes.driver.service.annotation.prometheus.io/scrape: "true"
    spark.kubernetes.driver.service.annotation.prometheus.io/path: "/metrics/driver/prometheus/"
    spark.kubernetes.driver.service.annotation.prometheus.io/port: "4040"
    spark.metrics.conf.*.sink.prometheusServlet.class: "org.apache.spark.metrics.sink.PrometheusServlet"
    spark.metrics.conf.*.sink.prometheusServlet.path: "/metrics/driver/prometheus/"
    spark.metrics.conf.master.sink.prometheusServlet.path: "/metrics/master/prometheus/"
    spark.metrics.conf.applications.sink.prometheusServlet.path: "/metrics/applications/prometheus/"

  sparkVersion: "3.3.1"
  restartPolicy:
    type: Never
  driver:
    cores: 2
    # Security Fix: Increased memory from 3g to 8g for 3TB TPCDS benchmark
    memory: "8g"
    # Replace <SERVICE_ACCOUNT> with your Kubernetes service account name
    serviceAccount: <SERVICE_ACCOUNT>
    # Using Karpenter provisioner nodeSelectors and tolerations
    nodeSelector:
      NodeGroupType: "SparkComputeOptimized"
    tolerations:
      - key: "spark-compute-optimized"
        operator: "Exists"
        effect: "NoSchedule"
  executor:
    cores: 8
    instances: 2
    memory: "16g"
    # Replace <SERVICE_ACCOUNT> with your Kubernetes service account name
    serviceAccount: <SERVICE_ACCOUNT>
    # Using Karpenter provisioner nodeSelectors and tolerations
    nodeSelector:
      NodeGroupType: "SparkComputeOptimized"
    tolerations:
      - key: "spark-compute-optimized"
        operator: "Exists"
        effect: "NoSchedule"
